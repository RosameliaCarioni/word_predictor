{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e404c1d-547f-47f5-b7e0-a0d322914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in /opt/conda/lib/python3.10/site-packages (1.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from datetime import datetime\n",
    "import time \n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, DistributedSampler, random_split\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.multiprocessing as mp\n",
    "from torchsummary import summary\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from data_loading_utils import read_lines_from_file_as_data_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c64ac2-e0f8-4fca-ae8b-b97e7b234c56",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94eb9737-07c4-4ab9-9c0e-ffdf4555d1f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading clean text from txt files to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    Datapoints are sequences of words (tokenized) + label (next token). If the \n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    chunk_size: how much we read from the file at the time - we could play around with it. \n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, tokenizer, samples_length=5, chunk_size=1000000, artificial_padding=True):\n",
    "        self.sequences = [] # X\n",
    "        self.labels = [] # Y \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples_length = samples_length\n",
    "        self.artificial_padding = artificial_padding\n",
    "        self.pad_token_id = tokenizer.pad_token_id  # Get the PAD token ID = 0 \n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.read_file, filename, chunk_size) for filename in filenames]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure all files are processed\n",
    "        # Convert lists to numpy arrays for faster access and better memory management\n",
    "        self.sequences = np.array(self.sequences)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def read_file(self, filename, chunk_size):\n",
    "        print(\"Read in \", filename)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            read_lines_from_file_as_data_chunks(filename, chunk_size, self.process_lines)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        end_time = time.time()  # End the timer\n",
    "        print(f\"Time taken to read {filename}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def process_lines(self, data, eof, file_name):\n",
    "        \"\"\"\n",
    "        eof: end of file \n",
    "        Callback function to process lines read from file.\n",
    "        \"\"\"\n",
    "        if not eof:\n",
    "            text = data.strip()  # Remove leading/trailing whitespace\n",
    "            # split sentence into sub-sentences so that it can be passed to tokenizer, which has a max capacity of 512 \n",
    "            line_chunks = self.split_into_chunks(text) \n",
    "            for chunk in line_chunks:\n",
    "                line_tokens = self.tokenizer.tokenize(chunk) # data is already lower case \n",
    "                line_tokens_ids = self.tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "                self.create_sequences(line_tokens_ids)\n",
    "        else:\n",
    "            print(f\"Finished reading file: {file_name}\")\n",
    "\n",
    "    def split_into_chunks(self, line, max_length=512):\n",
    "        \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "        return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "    def create_sequences(self, token_ids):\n",
    "        \"\"\"\n",
    "        Create sequences and labels from tokenized text.\n",
    "        \"\"\"\n",
    "        n = self.samples_length\n",
    "        if self.artificial_padding:\n",
    "            k = 0 \n",
    "            while k < len(token_ids) - n:\n",
    "                for i in range(1, n + 1):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)\n",
    "                k += n\n",
    "            remaining_tokens = len(token_ids) - k\n",
    "            if remaining_tokens > 1:\n",
    "                for i in range(1, remaining_tokens):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)     \n",
    "        else: \n",
    "            # Ensure all sequences are of length samples_length\n",
    "            for i in range(self.samples_length, len(token_ids)): # sliding window \n",
    "                seq = token_ids[i-self.samples_length:i]\n",
    "                label = token_ids[i]\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3bf47-749c-47f3-a89e-a7a69e815d8b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20db4c69-9e6c-47a5-b404-37152b511ae6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, device, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.1, last_output=True, max_len=5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.last_output = last_output\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        if last_output:\n",
    "            self.linear = nn.Linear(d_model, ntoken)\n",
    "        else:\n",
    "            self.linear = nn.Linear(d_model*max_len, ntoken)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[batch_size, seq_len]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[batch_size, seq_len, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        if self.last_output:\n",
    "            output = self.linear(output[:, -1, :])  # Take the last token's output\n",
    "            return output\n",
    "        else:\n",
    "            flattened_transf = output.reshape(src.size(0), 1, -1)  # Flatten the output\n",
    "            result = self.linear(torch.tanh(flattened_transf))\n",
    "            return result.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394eacb5-a619-465d-b5ca-89729047284d",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94c61f84-caa1-4811-a8cb-5805d31bfdbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device, criterion):\n",
    "    correct, incorrect, total_loss = 0, 0, 0\n",
    "    for seq, label in dataloader:\n",
    "        sequence, label = seq.to(device), label.to(device)\n",
    "        logits = model(sequence)\n",
    "        _, predicted_word_ids = logits.topk(1)\n",
    "        assert (label.shape == predicted_word_ids.squeeze(1).shape)\n",
    "        total_loss += criterion(logits.squeeze(1), label).item()\n",
    "        comparison = torch.eq(label, predicted_word_ids.squeeze(1))\n",
    "        count_same_entries = torch.sum(comparison).item()\n",
    "        #count_same_entries = (label == predicted_word_ids.squeeze(1)).sum().item()\n",
    "        \n",
    "        correct += count_same_entries\n",
    "        incorrect += label.shape[0] - count_same_entries\n",
    "\n",
    "    print( \"Correctly predicted words    : \", correct )\n",
    "    print( \"Incorrectly predicted words  : \", incorrect )\n",
    "    print( \"Accuracy                     : \", correct / (correct + incorrect))\n",
    "    print( \"PPL                          : \", math.exp(total_loss/(correct + incorrect)))\n",
    "    \n",
    "    return correct / (correct + incorrect), math.exp(total_loss/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb6c0a-aaa5-476c-936d-13b2c89d3eb4",
   "metadata": {},
   "source": [
    "# Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b94eea-0cda-43f2-9711-73ac8214cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b3088f4-fc35-4607-98a9-345218b233ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in  data/clean_data/articles.txt\n",
      "Finished reading file: data/clean_data/articles.txt\n",
      "Time taken to read data/clean_data/articles.txt: 1.91 seconds\n"
     ]
    }
   ],
   "source": [
    "# select files with text for training (will also be used for test and validation dataset)\n",
    "filenames = ['data/clean_data/articles.txt'] #,'data/clean_data/twitter.txt', 'data/clean_data/news_summarization.txt']\n",
    "\n",
    "# choose tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  \n",
    "dataset = WPDataset(filenames=filenames, tokenizer=tokenizer, samples_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09eadbb-a175-49f8-8103-6c45adc3f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  699865 total number of sables in the dataset.\n",
      "There are:  559893  training datapoints and  30522 unique tokens in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# set up dataloaders\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "training_data, validation_data, test_data = random_split(dataset, [0.8, 0.05, 0.15], generator=generator)\n",
    "\n",
    "print(\"There are: \", len(dataset), \"total number of samples in the dataset.\")\n",
    "print( \"There are: \", len(training_data), \" training datapoints and \", tokenizer.vocab_size, \"unique tokens in the vocabulary\" ) \n",
    "print( \"There are: \", len(validation_data), \" validation datapoints\")\n",
    "print( \"There are: \", len(test_data), \" testing datapoints\")\n",
    "\n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4962652-fcf4-4b51-a054-59de4f282018",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b06c5c-be69-4923-9207-226094fd7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_epoch = 2\n",
    "\n",
    "ntoken = tokenizer.vocab_size  # Vocabulary size - BERT\n",
    "num_processes = 4\n",
    "d_model = 64  # Model dimension\n",
    "nhead = 8  # Number of attention heads 12 | 8\n",
    "d_hid = 512  # Hidden dimension in the feedforward layer - or maybe even 256? often it's 4*d_model\n",
    "nlayers = 3  # Number of transformer layers\n",
    "dropout = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aeaafd-f38a-4598-8be8-54576f5d616b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f145539-d102-4dea-875c-965696cdb5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(5719)\n",
    "\n",
    "device = (\n",
    "\"cuda\"\n",
    "if torch.cuda.is_available()\n",
    "else \"mps\"\n",
    "if torch.backends.mps.is_available()\n",
    "else \"cpu\"\n",
    ")\n",
    "print( \"Running on\", device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f82b95-9864-4e81-a701-6b4eb5ab371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─PositionalEncoding: 1-1                     --\n",
      "|    └─Dropout: 2-1                           --\n",
      "├─TransformerEncoder: 1-2                     --\n",
      "|    └─ModuleList: 2-2                        --\n",
      "|    |    └─TransformerEncoderLayer: 3-1      83,008\n",
      "|    |    └─TransformerEncoderLayer: 3-2      83,008\n",
      "|    |    └─TransformerEncoderLayer: 3-3      83,008\n",
      "├─Embedding: 1-3                              1,953,408\n",
      "├─Linear: 1-4                                 1,983,930\n",
      "======================================================================\n",
      "Total params: 4,186,362\n",
      "Trainable params: 4,186,362\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─PositionalEncoding: 1-1                     --\n",
       "|    └─Dropout: 2-1                           --\n",
       "├─TransformerEncoder: 1-2                     --\n",
       "|    └─ModuleList: 2-2                        --\n",
       "|    |    └─TransformerEncoderLayer: 3-1      83,008\n",
       "|    |    └─TransformerEncoderLayer: 3-2      83,008\n",
       "|    |    └─TransformerEncoderLayer: 3-3      83,008\n",
       "├─Embedding: 1-3                              1,953,408\n",
       "├─Linear: 1-4                                 1,983,930\n",
       "======================================================================\n",
       "Total params: 4,186,362\n",
       "Trainable params: 4,186,362\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model \n",
    "model = TransformerModel(device, ntoken=ntoken, d_model=d_model, nhead=nhead, nlayers=nlayers, \n",
    "                         d_hid=d_hid, dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee8f1226-8932-4f63-ab65-61d3a2673ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a checkpoint exists\n",
    "path_to_load = 'weights/transformer_news_articles.pt'\n",
    "\n",
    "if os.path.exists(path_to_load):\n",
    "    # Load the saved model state\n",
    "    model.load_state_dict(torch.load(path_to_load))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead116d6-4ccc-4c39-b66f-7dab70bfdff0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 692890/1368650 [12:23:06<12:04:44, 15.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m#clip_grad_norm_(model.parameters(), 5) TODO \u001b[39;00m\n\u001b[1;32m     16\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, total_loss)\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m    \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "#prev_accuracy = 0\n",
    "path_to_save = 'weights/transformer_news_articles_twitter.pt'\n",
    "for epoch in range(n_epoch):\n",
    "    total_loss = 0\n",
    "    hidden = None\n",
    "    with tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch + 1), mininterval=50) as tepoch:\n",
    "        for sequence, label in tepoch:\n",
    "            sequence, label = sequence.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(sequence)       \n",
    "            loss = criterion(logits.squeeze(1), label)\n",
    "            loss.backward()\n",
    "            #clip_grad_norm_(model.parameters(), 5) TODO \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    print(\"Epoch \", epoch+1, \" loss: \", total_loss)\n",
    "    total_loss = 0    \n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(), path_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e721d82-d366-4c5c-9386-17450ad1761a",
   "metadata": {},
   "source": [
    "# Evaluation on test set - Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21508f97-035c-4530-9a90-fb15edab1e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the test data...\n",
      "Number of test sentences:  1640\n",
      "\n",
      "Correctly predicted words    :  12315\n",
      "Incorrectly predicted words  :  92645\n",
      "Accuracy                     :  0.11733041158536585\n",
      "PPL                          :  1.113897906775641\n",
      "Test acc:  0.11733041158536585\n",
      "ppl:  1.113897906775641\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print( \"Evaluating on the test data...\" )\n",
    "test_accuracy, ppl = evaluate(test_dataloader, model, device, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
