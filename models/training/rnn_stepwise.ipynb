{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e404c1d-547f-47f5-b7e0-a0d322914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in /opt/conda/lib/python3.10/site-packages (1.4.5)\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, DistributedSampler, random_split\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "!pip install torch-summary\n",
    "from torchsummary import summary\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from data_loading_utils import read_lines_from_file_as_data_chunks\n",
    "import time  # Import the time module\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94eb9737-07c4-4ab9-9c0e-ffdf4555d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading clean text from txt files to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    Datapoints are sequences of words (tokenized) + label (next token). If the \n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    chunk_size: how much we read from the file at the time - we could play around with it. \n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, tokenizer, samples_length=5, chunk_size=1000000, artificial_padding=True):\n",
    "        self.sequences = [] # X\n",
    "        self.labels = [] # Y \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples_length = samples_length\n",
    "        self.artificial_padding = artificial_padding\n",
    "        self.pad_token_id = tokenizer.pad_token_id  # Get the PAD token ID = 0 \n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.read_file, filename, chunk_size) for filename in filenames]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure all files are processed\n",
    "        # Convert lists to numpy arrays for faster access and better memory management\n",
    "        self.sequences = np.array(self.sequences)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def read_file(self, filename, chunk_size):\n",
    "        print(\"Read in \", filename)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            read_lines_from_file_as_data_chunks(filename, chunk_size, self.process_lines)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        end_time = time.time()  # End the timer\n",
    "        print(f\"Time taken to read {filename}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def process_lines(self, data, eof, file_name):\n",
    "        \"\"\"\n",
    "        eof: end of file \n",
    "        Callback function to process lines read from file.\n",
    "        \"\"\"\n",
    "        if not eof:\n",
    "            text = data.strip()  # Remove leading/trailing whitespace\n",
    "            # split sentence into sub-sentences so that it can be passed to tokenizer, which has a max capacity of 512 \n",
    "            line_chunks = self.split_into_chunks(text) \n",
    "            for chunk in line_chunks:\n",
    "                line_tokens = self.tokenizer.tokenize(chunk) # data is already lower case \n",
    "                line_tokens_ids = self.tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "                self.create_sequences(line_tokens_ids)\n",
    "        else:\n",
    "            print(f\"Finished reading file: {file_name}\")\n",
    "\n",
    "    def split_into_chunks(self, line, max_length=512):\n",
    "        \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "        return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "    def create_sequences(self, token_ids):\n",
    "        \"\"\"\n",
    "        Create sequences and labels from tokenized text.\n",
    "        \"\"\"\n",
    "        n = self.samples_length\n",
    "        if self.artificial_padding:\n",
    "            k = 0 \n",
    "            while k < len(token_ids) - n:\n",
    "                for i in range(1, n + 1):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)\n",
    "                k += n\n",
    "            remaining_tokens = len(token_ids) - k\n",
    "            if remaining_tokens > 1:\n",
    "                for i in range(1, remaining_tokens):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)     \n",
    "        else: \n",
    "            # Ensure all sequences are of length samples_length\n",
    "            for i in range(self.samples_length, len(token_ids)): # sliding window \n",
    "                seq = token_ids[i-self.samples_length:i]\n",
    "                label = token_ids[i]\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05745081-1832-4def-8c9a-d6b90862bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    There are two possible ways to write this class; either it tries to predict \n",
    "    a whole word that consists of several tokens or it only predicts the next token\n",
    "    after a fixed (or variable) amount of input tokens; \n",
    "    Another choice is whether to use a hidden state or not as an input to the forward pass\n",
    "    Or do a encoder - decoder structure?\n",
    "\n",
    "    I read somewhere that it is good to ... \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, hidden_size, no_of_output_symbols, device, num_layers, use_GRU, dropout):\n",
    "        super().__init__()\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.use_GRU = use_GRU\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # initialize layers\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols, embedding_size)\n",
    "        if use_GRU == True:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear( hidden_size, no_of_output_symbols )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size, max_seq_length)\n",
    "        Each inner list contains word IDs and represents one datapoint (n words).\n",
    "       \n",
    "        Returns:\n",
    "        the output from the RNN: logits for the predicted next word, hidden state\n",
    "        \"\"\"\n",
    "        x_emb = self.embedding(x) # x_emb shape: (batch_size, max_seq_length, emb_dim)\n",
    "        if self.use_GRU:\n",
    "            output, hidden = self.rnn(x_emb, hidden) # output shape: (batch_size, max_seq_length, hidden)\n",
    "        else:\n",
    "            output, (h_n, c_n) = self.rnn(x_emb, hidden)  # LSTM expects a tuple (hidden state, cell state)\n",
    "            hidden = (h_n, c_n)\n",
    "            \n",
    "        return self.output(output[:, -1, :]), hidden # logit shape: (batch_size, 1, vocab_size)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec608b05-644d-44cb-8764-8e112736941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def pad_sequence(batch, pad_symbol): #=tokenizer.pad_token):\n",
    "    \"\"\"\n",
    "    Applies padding if the number of tokens in sequences differs within one batch.\n",
    "    Only applies padding to the sequence, not the label.\n",
    "    \"\"\"\n",
    "    seq, label = zip(*batch)\n",
    "    max_seq_len = max(map(len, seq))\n",
    "    max_label_len = max(map(len, label))\n",
    "    padded_seq = [[b[i] if i < len(b) else pad_symbol for i in range(max_seq_len)] for b in seq]\n",
    "    padded_label = [[l[i] if i < len(l) else pad_symbol for i in range(max_label_len)] for l in label]\n",
    "    return padded_seq, padded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be662011-fee9-464d-93eb-f51e87e2ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, rnn_model, device):\n",
    "    correct, incorrect = 0,0\n",
    "    hidden = None\n",
    "    for seq, label in dataloader:\n",
    "        sequence, label = seq.to(device), label.to(device)\n",
    "        prediction, _ = rnn_model(sequence, hidden)\n",
    "        _, predicted_tensor = prediction.topk(1)\n",
    "\n",
    "        \n",
    "        assert (label.shape == predicted_tensor.squeeze(1).shape)\n",
    "        comparison = torch.eq(label, predicted_tensor.squeeze(1))\n",
    "        count_same_entries = torch.sum(comparison).item()\n",
    "        count_same_entries = (label == predicted_tensor.squeeze(1)).sum().item()\n",
    "        \n",
    "        correct += count_same_entries\n",
    "        incorrect += label.shape[0] - count_same_entries\n",
    "\n",
    "    print( \"Correctly predicted words    : \", correct )\n",
    "    print( \"Incorrectly predicted words  : \", incorrect )\n",
    "    print( \"Accuracy                     : \", correct / (correct + incorrect))\n",
    "    \n",
    "    return correct / (correct + incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead116d6-4ccc-4c39-b66f-7dab70bfdff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Read in  data/clean_data/news_summarization.txt\n",
      "Read in  data/clean_data/twitter.txt\n",
      "Finished reading file: data/clean_data/news_summarization.txt\n",
      "Time taken to read data/clean_data/news_summarization.txt: 231.04 seconds\n",
      "Finished reading file: data/clean_data/twitter.txt\n",
      "Time taken to read data/clean_data/twitter.txt: 481.05 seconds\n",
      "There are 139495661  training datapoints and  28996 unique tokens in the dataset\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Embedding: 1-1                         1,449,800\n",
      "├─GRU: 1-2                               47,232\n",
      "├─Linear: 1-3                            1,884,740\n",
      "=================================================================\n",
      "Total params: 3,381,772\n",
      "Trainable params: 3,381,772\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2179619/2179619 [1:36:45<00:00, 375.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 13053926.910126686\n",
      "Evaluating on the validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted words    :  1294340\n",
      "Incorrectly predicted words  :  7424124\n",
      "Accuracy                     :  0.14845963692687153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2179619/2179619 [1:35:04<00:00, 382.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 12997308.090759754\n",
      "Evaluating on the validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted words    :  1272151\n",
      "Incorrectly predicted words  :  7446313\n",
      "Accuracy                     :  0.14591457853126422\n",
      "Correctly predicted words    :  1272061\n",
      "Incorrectly predicted words  :  7446403\n",
      "Accuracy                     :  0.1459042556119977\n",
      "Evaluating on the test data...\n",
      "Number of test sentences:  408678\n",
      "\n",
      "Correctly predicted words    :  4067826\n",
      "Incorrectly predicted words  :  22087566\n",
      "Accuracy                     :  0.15552533106749078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ================ Hyper-parameters ================ #\n",
    "\n",
    "batch_size = 64\n",
    "embedding_size = 50 #16\n",
    "hidden_size = 64 #25\n",
    "num_layers = 2\n",
    "seq_length = 5      # number of tokens used as a datapoint\n",
    "learning_rate = 0.001\n",
    "epochs = 2\n",
    "num_processes = 4\n",
    "use_GRU = True\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "# ====================== Data ===================== #\n",
    "\n",
    "# select files with text for training (will also be used for test and validation dataset)\n",
    "txt_files = [\"articles.txt\", \"news_summarization.txt\"]\n",
    "filenames = ['data/clean_data/news_summarization.txt', 'data/clean_data/twitter.txt'] #'data/clean_data/mobile_text.txt',  'data/clean_data/articles.txt' 'data/clean_data/news_summarization.txt', , 'data/clean_data/twitter.txt' \n",
    "\n",
    "# choose tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# ==================== Training ==================== #\n",
    "# Reproducibility\n",
    "np.random.seed(5719)\n",
    "\n",
    "device = (\n",
    "\"cuda\"\n",
    "if torch.cuda.is_available()\n",
    "else \"mps\"\n",
    "if torch.backends.mps.is_available()\n",
    "else \"cpu\"\n",
    ")\n",
    "print( \"Running on\", device )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set up dataloaders\n",
    "dataset = WPDataset(filenames=filenames, tokenizer=tokenizer, samples_length=seq_length)\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "training_data, validation_data, test_data = random_split(dataset, [0.8, 0.05, 0.15], generator=generator)\n",
    "\n",
    "print( \"There are\", len(training_data), \" training datapoints and \", tokenizer.vocab_size, \"unique tokens in the dataset\" ) \n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "\n",
    "\n",
    "rnn_model = RNN(embedding_size, hidden_size, no_of_output_symbols=tokenizer.vocab_size, device=device, num_layers=num_layers, use_GRU=use_GRU, dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "summary(rnn_model)\n",
    "\n",
    "# Check if a checkpoint exists\n",
    "checkpoint_path = 'model_checkpoint_twitter_first.pth'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the saved model state\n",
    "    rnn_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "#checkpoint_path = 'model_checkpoint.pth'\n",
    "\n",
    "rnn_model.train()\n",
    "\n",
    "prev_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    hidden = None\n",
    "    with tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch + 1)) as tepoch:\n",
    "        for sequence, label in tepoch:\n",
    "            sequence, label = sequence.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, hidden = rnn_model(sequence, hidden)\n",
    "            if use_GRU:\n",
    "                hidden = hidden.detach()  # Detach hidden states to avoid backprop through the entire sequence\n",
    "            else: \n",
    "                hidden = tuple([h.detach() for h in hidden])    \n",
    "            loss = criterion(logits.squeeze(), label)\n",
    "            loss.backward()\n",
    "            \n",
    "            clip_grad_norm_(rnn_model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    print(\"Epoch\", epoch, \"loss:\", total_loss )\n",
    "    total_loss = 0\n",
    "    torch.save(rnn_model, checkpoint_path)\n",
    "    print(\"Evaluating on the validation data...\")\n",
    "    accuracy = evaluate(val_dataloader, rnn_model, device)\n",
    "    if accuracy > prev_accuracy:\n",
    "        prev_accuracy = accuracy\n",
    "        torch.save(rnn_model, 'best_model_so_far_twitter_first.pth')\n",
    "\n",
    "\n",
    "# Save the model state\n",
    "torch.save(rnn_model.state_dict(), checkpoint_path)\n",
    "evaluate(val_dataloader, rnn_model, device)\n",
    "\n",
    "# ==================== Evaluation ==================== #\n",
    "\n",
    "rnn_model.eval()\n",
    "print( \"Evaluating on the test data...\" )\n",
    "\n",
    "print( \"Number of test sentences: \", len(test_dataloader) )\n",
    "print()\n",
    "\n",
    "test_accuracy = evaluate(test_dataloader, rnn_model, device)\n",
    "\n",
    "# ==================== Save the model  ==================== #\n",
    "\n",
    "dt = str(datetime.now()).replace(' ','_').replace(':','_').replace('.','_')\n",
    "newdir = 'model_' + dt\n",
    "os.mkdir( newdir )\n",
    "#torch.save( rnn_model.state_dict(), os.path.join(newdir, 'rnn.model') )\n",
    "torch.save( rnn_model, os.path.join(newdir, 'rnn.model') )\n",
    "\n",
    "settings = {\n",
    "    'epochs': epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'use_GRU': use_GRU,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "with open( os.path.join(newdir, 'settings.json'), 'w' ) as f:\n",
    "    json.dump(settings, f)\n",
    "\n",
    "s = f\"accuracy: {test_accuracy}, epochs: {epochs}, num_layers: {num_layers}, use_GRU: {use_GRU}, dropout: {dropout}, embedding_size: {embedding_size}, hidden_size: {hidden_size}, batch_size: {batch_size}, learning_rate: {learning_rate}\"\n",
    "with open(\"experiments.txt\", 'a') as f:\n",
    "    f.write(s + '\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afd6cb7-fa6d-4975-9020-1f11bd595a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_data.indices, 'val_indices_twitter_first.pt')\n",
    "torch.save(training_data.indices, 'train_indices_twitter_first.pt')\n",
    "torch.save(test_data.indices, 'test_indices_twitter_first.pt')\n",
    "\n",
    "train_indices = torch.load('train_indices_twitter_first.pt')\n",
    "val_indices = torch.load('val_indices_twitter_first.pt')\n",
    "test_indices = torch.load('test_indices_twitter_first.pt')\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6d48e-ae0b-43ee-98ae-23e66150aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNpredictor:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def filter_vocab_by_prefix(self, vocab, prefix):\n",
    "        if prefix == None:\n",
    "            return vocab\n",
    "        return {token: idx for token, idx in vocab.items() if token.startswith(prefix)}\n",
    "\n",
    "    def mask_logits_by_vocab(self, logits, filtered_vocab):\n",
    "        mask = torch.full_like(logits, float('-inf'))\n",
    "        for token, idx in filtered_vocab.items():\n",
    "            mask[idx] = logits[idx]\n",
    "        return mask\n",
    "    \n",
    "    def remove_last_word(self, input_string):\n",
    "        last_space_index = input_string.rfind(' ')\n",
    "        if last_space_index == -1:\n",
    "            return input_string, None\n",
    "        else:\n",
    "            return input_string[:last_space_index], input_string[last_space_index+1:]\n",
    "\n",
    "    def predict_next_word(self, prompt, number_of_suggestions, max_subwords=5):\n",
    "        self.model.eval()\n",
    "\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        hidden = None\n",
    "\n",
    "        # remove last word from prompt (word that is supposed to be predicted)\n",
    "        prompt, prefix = self.remove_last_word(prompt)\n",
    "        tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        \n",
    "        next_words = []\n",
    "\n",
    "        for i in range(number_of_suggestions):\n",
    "            generated_subwords = []\n",
    "            for _ in range(max_subwords):\n",
    "                with torch.no_grad():\n",
    "                    outputs, hidden = self.model(input_ids, hidden)\n",
    "                    next_token_logits = outputs.squeeze()  # Get the logits for the last token\n",
    "                    next_token_id = torch.argmax(next_token_logits, dim=-1).item()  # Get the ID of the highest probability token\n",
    "                    next_token_ids = next_token_logits.topk(5).indices.tolist()\n",
    "\n",
    "                if len(generated_subwords) == 0:\n",
    "                    # filter by prefix\n",
    "                    filtered_vocab = self.filter_vocab_by_prefix(vocab, prefix)\n",
    "                    # Mask the logits based on the filtered vocabulary\n",
    "                    masked_logits = self.mask_logits_by_vocab(next_token_logits, filtered_vocab)\n",
    "                    # Normalize the masked logits to get probabilities\n",
    "                    probs = torch.softmax(masked_logits, dim=-1)\n",
    "                    #next_token_id = torch.argmax(probs, dim=-1).item()\n",
    "                    next_token_id = probs.topk(5).indices.tolist()[i]\n",
    "                else: \n",
    "                    next_token_id = next_token_logits.topk(5).indices.tolist()[i]\n",
    "\n",
    "\n",
    "                # Decode the generated subwords so far\n",
    "                subword_text = self.tokenizer.decode([next_token_id], clean_up_tokenization_spaces=True)\n",
    "                # Check if the last token completes a word\n",
    "                if not subword_text.startswith(\"##\") and len(generated_subwords) > 0:  # Check if it's not a continuation of a word\n",
    "                    break\n",
    "\n",
    "                generated_subwords.append(next_token_id)\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]]).to(self.device)], dim=1).to(self.device)  # Append the predicted token to the input\n",
    "\n",
    "            # Decode the generated subwords to form the next word\n",
    "            next_word = self.tokenizer.decode(generated_subwords, clean_up_tokenization_spaces=True).strip()\n",
    "            next_words.append(next_word)\n",
    "\n",
    "        return next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a8183-fe1a-44fb-91d6-2661e449c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 50 #16\n",
    "hidden_size = 64 #25\n",
    "seq_length = 5      # number of tokens used as a datapoint\n",
    "learning_rate = 0.001\n",
    "epochs = 2\n",
    "num_layers = 2\n",
    "num_processes = 4\n",
    "GRU = False\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = (\n",
    "\"cuda\"\n",
    "if torch.cuda.is_available()\n",
    "else \"mps\"\n",
    "if torch.backends.mps.is_available()\n",
    "else \"cpu\"\n",
    ")\n",
    "print( \"Running on\", device )\n",
    "\n",
    "#rnn_model = RNN(embedding_size, hidden_size, no_of_output_symbols=tokenizer.vocab_size, device=device, num_layers=num_layers, use_GRU = use_GRU).to(device)\n",
    "#rnn_model.load_state_dict(torch.load('model_2024-05-28_06_59_20_927544/rnn.model'))    \n",
    "rnn_model = torch.load('LSTM_large/rnn.model')\n",
    "rnn = RNNpredictor(rnn_model, tokenizer, device)\n",
    "\n",
    "rnn.predict_next_word(\"on top of the world there are name\", 5)\n",
    "rnn.predict_next_word(\"I am from Ge\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ee8bf-78d7-403a-8454-7eff4af52863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a94ac-5a37-4313-a97c-5d8d1cc840f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
