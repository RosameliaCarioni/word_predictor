{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e404c1d-547f-47f5-b7e0-a0d322914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in /opt/conda/lib/python3.10/site-packages (1.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, DistributedSampler, random_split\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "!pip install torch-summary\n",
    "from torchsummary import summary\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "\n",
    "from data_loading_utils import read_lines_from_file_as_data_chunks\n",
    "import time  # Import the time module\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94eb9737-07c4-4ab9-9c0e-ffdf4555d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading clean text from txt files to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    Datapoints are sequences of words (tokenized) + label (next token). If the \n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    chunk_size: how much we read from the file at the time - we could play around with it. \n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, tokenizer, samples_length=5, chunk_size=1000000, artificial_padding=True):\n",
    "        self.sequences = [] # X\n",
    "        self.labels = [] # Y \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples_length = samples_length\n",
    "        self.artificial_padding = artificial_padding\n",
    "        self.pad_token_id = tokenizer.pad_token_id  # Get the PAD token ID = 0 \n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.read_file, filename, chunk_size) for filename in filenames]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure all files are processed\n",
    "        # Convert lists to numpy arrays for faster access and better memory management\n",
    "        self.sequences = np.array(self.sequences)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def read_file(self, filename, chunk_size):\n",
    "        print(\"Read in \", filename)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            read_lines_from_file_as_data_chunks(filename, chunk_size, self.process_lines)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        end_time = time.time()  # End the timer\n",
    "        print(f\"Time taken to read {filename}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def process_lines(self, data, eof, file_name):\n",
    "        \"\"\"\n",
    "        eof: end of file \n",
    "        Callback function to process lines read from file.\n",
    "        \"\"\"\n",
    "        if not eof:\n",
    "            text = data.strip()  # Remove leading/trailing whitespace\n",
    "            # split sentence into sub-sentences so that it can be passed to tokenizer, which has a max capacity of 512 \n",
    "            line_chunks = self.split_into_chunks(text) \n",
    "            for chunk in line_chunks:\n",
    "                line_tokens = self.tokenizer.tokenize(chunk) # data is already lower case \n",
    "                line_tokens_ids = self.tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "                self.create_sequences(line_tokens_ids)\n",
    "        else:\n",
    "            print(f\"Finished reading file: {file_name}\")\n",
    "\n",
    "    def split_into_chunks(self, line, max_length=512):\n",
    "        \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "        return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "    def create_sequences(self, token_ids):\n",
    "        \"\"\"\n",
    "        Create sequences and labels from tokenized text.\n",
    "        \"\"\"\n",
    "        n = self.samples_length\n",
    "        if self.artificial_padding:\n",
    "            k = 0 \n",
    "            while k < len(token_ids) - n:\n",
    "                for i in range(1, n + 1):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)\n",
    "                k += n\n",
    "            remaining_tokens = len(token_ids) - k\n",
    "            if remaining_tokens > 1:\n",
    "                for i in range(1, remaining_tokens):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)     \n",
    "        else: \n",
    "            # Ensure all sequences are of length samples_length\n",
    "            for i in range(self.samples_length, len(token_ids)): # sliding window \n",
    "                seq = token_ids[i-self.samples_length:i]\n",
    "                label = token_ids[i]\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05745081-1832-4def-8c9a-d6b90862bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network (RNN) with optional GRU or LSTM units.\n",
    "\n",
    "    Attributes:\n",
    "    - no_of_output_symbols (int): Size of the output vocabulary.\n",
    "    - embedding_size (int): Dimensionality of the embeddings.\n",
    "    - hidden_size (int): Number of features in the hidden state.\n",
    "    - num_layers (int): Number of recurrent layers.\n",
    "    - use_GRU (bool): If True, use GRU; otherwise, use LSTM.\n",
    "    - dropout (float): Dropout probability.\n",
    "    - device (torch.device): Device for the model ('cpu', 'mps' or 'cuda').\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, hidden_size, no_of_output_symbols, device, num_layers, use_GRU, dropout):\n",
    "        super().__init__()\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.use_GRU = use_GRU\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # initialize layers\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols, embedding_size)\n",
    "        if use_GRU == True:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear( hidden_size, no_of_output_symbols )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size, max_seq_length)\n",
    "        Each inner list contains word IDs and represents one datapoint (n tokens).\n",
    "       \n",
    "        Returns:\n",
    "        the output from the RNN: logits for the predicted next word, hidden state\n",
    "        \"\"\"\n",
    "        x_emb = self.embedding(x) # x_emb shape: (batch_size, max_seq_length, emb_dim)\n",
    "        if self.use_GRU:\n",
    "            output, hidden = self.rnn(x_emb, hidden) # output shape: (batch_size, max_seq_length, hidden)\n",
    "        else:\n",
    "            output, (h_n, c_n) = self.rnn(x_emb, hidden)  # LSTM expects a tuple (hidden state, cell state)\n",
    "            hidden = (h_n, c_n)\n",
    "            \n",
    "        return self.output(output[:, -1, :]), hidden # logit shape: (batch_size, 1, vocab_size)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be662011-fee9-464d-93eb-f51e87e2ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device, criterion):\n",
    "    correct, incorrect, total_loss = 0, 0, 0\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    for seq, label in dataloader:\n",
    "        sequence, label = seq.to(device), label.to(device)\n",
    "        logits, hidden = model(sequence, hidden)\n",
    "        _, predicted_word_ids = logits.topk(1)\n",
    "        assert (label.shape == predicted_word_ids.squeeze(1).shape)\n",
    "        total_loss += criterion(logits.squeeze(1), label).item()\n",
    "        comparison = torch.eq(label, predicted_word_ids.squeeze(1))\n",
    "        count_same_entries = torch.sum(comparison).item()\n",
    "        #count_same_entries = (label == predicted_word_ids.squeeze(1)).sum().item()\n",
    "        \n",
    "        correct += count_same_entries\n",
    "        incorrect += label.shape[0] - count_same_entries\n",
    "\n",
    "    print( \"Correctly predicted words    : \", correct )\n",
    "    print( \"Incorrectly predicted words  : \", incorrect )\n",
    "    print( \"Accuracy                     : \", correct / (correct + incorrect))\n",
    "    print( \"PPL                          : \", math.exp(total_loss/len(dataloader))\n",
    "    \n",
    "    return correct / (correct + incorrect), math.exp(total_loss/len(dataloader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ead116d6-4ccc-4c39-b66f-7dab70bfdff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Read in  data/clean_data/articles.txt\n",
      "Finished reading file: data/clean_data/articles.txt\n",
      "Time taken to read data/clean_data/articles.txt: 1.91 seconds\n",
      "There are 559893  training datapoints and  30522 unique tokens in the dataset\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Embedding: 1-1                         1,526,100\n",
      "├─GRU: 1-2                               47,232\n",
      "├─Linear: 1-3                            1,983,930\n",
      "=================================================================\n",
      "Total params: 3,557,262\n",
      "Trainable params: 3,557,262\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 8748/8748 [00:27<00:00, 320.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 52778.69981575012\n",
      "Evaluating on the validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted words    :  4463\n",
      "Incorrectly predicted words  :  30481\n",
      "Accuracy                     :  0.12771863553113552\n",
      "PPL                          :  1.1003528007311816\n",
      "Evaluating on the test data...\n",
      "Number of test sentences:  1640\n",
      "\n",
      "Correctly predicted words    :  13565\n",
      "Incorrectly predicted words  :  91395\n",
      "Accuracy                     :  0.12923971036585366\n",
      "PPL                          :  1.100034508706035\n"
     ]
    }
   ],
   "source": [
    "# ================ Hyper-parameters ================ #\n",
    "\n",
    "batch_size = 64\n",
    "embedding_size = 50 #16\n",
    "hidden_size = 64 #25\n",
    "num_layers = 2\n",
    "seq_length = 5      # number of tokens used as a datapoint\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "num_processes = 4\n",
    "use_GRU = True\n",
    "dropout = 0.5\n",
    "artificial_padding = True\n",
    "\n",
    "# ====================== Data ===================== #\n",
    "\n",
    "# select files with text for training (will also be used for test and validation dataset)\n",
    "filenames = ['data/clean_data/news_summarization.txt'] \n",
    "filenames = ['data/clean_data/twitter.txt']\n",
    "filenames = ['data/clean_data/articles.txt']\n",
    "\n",
    "# choose tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(5719)\n",
    "\n",
    "device = (\n",
    "\"cuda\"\n",
    "if torch.cuda.is_available()\n",
    "else \"mps\"\n",
    "if torch.backends.mps.is_available()\n",
    "else \"cpu\"\n",
    ")\n",
    "print( \"Running on\", device )\n",
    "\n",
    "# set up dataloaders\n",
    "dataset = WPDataset(filenames=filenames, tokenizer=tokenizer, samples_length=seq_length, artificial_padding=artificial_padding)\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "training_data, validation_data, test_data = random_split(dataset, [0.8, 0.05, 0.15], generator=generator)\n",
    "\n",
    "print( \"There are\", len(training_data), \" training datapoints and \", tokenizer.vocab_size, \"unique tokens in the dataset\" ) \n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "\n",
    "# ==================== Training ==================== #\n",
    "\n",
    "rnn_model = RNN(embedding_size, hidden_size, no_of_output_symbols=tokenizer.vocab_size, device=device, num_layers=num_layers, use_GRU=use_GRU, dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "summary(rnn_model)\n",
    "\n",
    "# Check if a checkpoint exists\n",
    "checkpoint_path = 'best_model.pth'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the saved model state\n",
    "    rnn_model = torch.load(checkpoint_path)\n",
    "\n",
    "prev_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    hidden = None\n",
    "    with tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch + 1)) as tepoch:\n",
    "        rnn_model.train()\n",
    "        for sequence, label in tepoch:\n",
    "            sequence, label = sequence.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, hidden = rnn_model(sequence, hidden)\n",
    "            if use_GRU:\n",
    "                hidden = hidden.detach()  # Detach hidden states to avoid backprop through the entire sequence\n",
    "            else: \n",
    "                hidden = tuple([h.detach() for h in hidden])    \n",
    "            loss = criterion(logits.squeeze(), label)\n",
    "            loss.backward()\n",
    "            \n",
    "            clip_grad_norm_(rnn_model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    print(\"Epoch\", epoch, \"loss:\", total_loss )\n",
    "    total_loss = 0\n",
    "    torch.save(rnn_model, checkpoint_path)\n",
    "    print(\"Evaluating on the validation data...\")\n",
    "    accuracy, ppl = evaluate(val_dataloader, rnn_model, device, criterion)\n",
    "    if accuracy > prev_accuracy:\n",
    "        prev_accuracy = accuracy\n",
    "        path = 'best_model.pth'\n",
    "        torch.save(rnn_model, path)\n",
    "\n",
    "\n",
    "# Save the model state\n",
    "checkpoint_path = 'checkpoint_path_epoch_' + str(epoch) + '.pth'\n",
    "torch.save(rnn_model, checkpoint_path)\n",
    "\n",
    "# ==================== Evaluation ==================== #\n",
    "\n",
    "rnn_model.eval()\n",
    "print( \"Evaluating on the test data...\" )\n",
    "\n",
    "print( \"Number of test sentences: \", len(test_dataloader) )\n",
    "print()\n",
    "\n",
    "test_accuracy, ppl = evaluate(test_dataloader, rnn_model, device, criterion)\n",
    "\n",
    "# ==================== Save the model  ==================== #\n",
    "\n",
    "dt = str(datetime.now()).replace(' ','_').replace(':','_').replace('.','_')\n",
    "newdir = 'model_' + dt\n",
    "os.mkdir( newdir )\n",
    "torch.save( rnn_model, os.path.join(newdir, 'rnn.model') )\n",
    "\n",
    "settings = {\n",
    "    'epochs': epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'embedding_size': embedding_size,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'use_GRU': use_GRU,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "with open( os.path.join(newdir, 'settings.json'), 'w' ) as f:\n",
    "    json.dump(settings, f)\n",
    "\n",
    "s = f\"accuracy: {test_accuracy}, epochs: {epochs}, num_layers: {num_layers}, use_GRU: {use_GRU}, dropout: {dropout}, embedding_size: {embedding_size}, hidden_size: {hidden_size}, batch_size: {batch_size}, learning_rate: {learning_rate}\"\n",
    "with open(\"experiments.txt\", 'a') as f:\n",
    "    f.write(s + '\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf54fd-a13a-47cc-af61-063c8a39d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Experiment 1  ==================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21119c3-9966-4a85-8250-e7fe0e1cca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "class RNNpredictor:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def filter_vocab_by_prefix(self, vocab, prefix):\n",
    "        if prefix == None:\n",
    "            return vocab\n",
    "        # return {token: idx for token, idx in vocab.items() if token.startswith(prefix)}\n",
    "        filtered_vocab = {}\n",
    "        filtered_subtokens = {}\n",
    "        for token, idx in vocab.items():\n",
    "            if token.startswith(prefix):\n",
    "                filtered_vocab[token] = idx\n",
    "            elif token.startswith('##') and token[2:].isalpha():\n",
    "                filtered_subtokens[token] = idx\n",
    "        return filtered_vocab, filtered_subtokens\n",
    "\n",
    "    def mask_logits_by_vocab(self, logits, filtered_vocab):\n",
    "        mask = torch.full_like(logits, float('-inf'))\n",
    "        for token, idx in filtered_vocab.items():\n",
    "            mask[idx] = logits[idx]\n",
    "        return mask\n",
    "\n",
    "    def mask_logits_by_subword(self, mask, logits, filtered_subwords):\n",
    "        for token, idx in filtered_subwords.items():\n",
    "            mask[idx] = logits[idx]\n",
    "        return mask\n",
    "\n",
    "    def remove_last_word(self, input_string, cut=True):\n",
    "        last_space_index = input_string.rfind(' ')\n",
    "        if last_space_index == -1:\n",
    "            return None, input_string.lower()\n",
    "        else:\n",
    "            if cut:\n",
    "                # if prompt is longer than seven words, cut it\n",
    "                words = input_string.lower().split()\n",
    "                last_seven_words = words[-7:]\n",
    "                result = ' '.join(last_seven_words)\n",
    "                if input_string[-1] == \" \":\n",
    "                    result += \" \"\n",
    "                last_space_index = result.rfind(' ')\n",
    "                input_string = result\n",
    "            return input_string[:last_space_index], input_string[last_space_index + 1:]\n",
    "\n",
    "    def predict_next_word(self, prompt, number_of_suggestions, max_subwords=3):\n",
    "        self.model.eval()\n",
    "\n",
    "        input_text = prompt\n",
    "        hidden = None\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        #nltk.download('words')\n",
    "        english_words = set(words.words())\n",
    "\n",
    "        # remove last word from prompt (word that is supposed to be predicted)\n",
    "        prompt, prefix = self.remove_last_word(prompt, True)\n",
    "        full_prompt, _ = self.remove_last_word(input_text, False)\n",
    "        if prompt == None:\n",
    "            tokens = [self.tokenizer.cls_token_id]\n",
    "        else:\n",
    "            tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        input_ids_start = torch.tensor(tokens).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        input_ids = input_ids_start\n",
    "\n",
    "\n",
    "        first_pass = []\n",
    "        suggestions = []\n",
    "        i = 0\n",
    "        while len(suggestions) < number_of_suggestions:\n",
    "            generated_subwords = []\n",
    "            for subword in range(max_subwords):\n",
    "                if len(suggestions) == 0 and i == 0:\n",
    "                    with torch.no_grad():\n",
    "                        outputs, hidden = self.model(input_ids, hidden)\n",
    "                        next_token_logits = outputs.squeeze()  # Get the logits for the last token\n",
    "\n",
    "                    # filter by prefix\n",
    "                    filtered_vocab, filtered_subwords = self.filter_vocab_by_prefix(vocab, prefix)\n",
    "                    # Mask the logits based on the filtered vocabulary\n",
    "                    masked_logits = self.mask_logits_by_vocab(next_token_logits, filtered_vocab)\n",
    "                    # Mask the logits for most common '##'\n",
    "                    masked_logits = self.mask_logits_by_subword(masked_logits, next_token_logits, filtered_subwords)\n",
    "                    # Normalize the masked logits to get probabilities\n",
    "                    probs = torch.softmax(masked_logits, dim=-1)\n",
    "                    first_pass = probs.topk(len(filtered_vocab)+len(filtered_subwords)).indices.tolist()\n",
    "                    next_token_id = first_pass[i]\n",
    "                elif len(generated_subwords) == 0:\n",
    "                    next_token_id = first_pass[i]\n",
    "                else:\n",
    "                    # filter by prefix\n",
    "                    filtered_vocab, _ = self.filter_vocab_by_prefix(vocab, generated_subwords[-1])\n",
    "                    if len(filtered_vocab) == 0:\n",
    "                        break\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs, hidden = self.model(input_ids, hidden)\n",
    "                        next_token_logits = outputs.squeeze()  # Get the logits for the last toke\n",
    "\n",
    "                    # Mask the logits based on the filtered vocabulary\n",
    "                    masked_logits = self.mask_logits_by_vocab(next_token_logits, filtered_vocab)\n",
    "                    # Find most likely end\n",
    "                    next_token_id = masked_logits.topk(1).indices.tolist()[0]\n",
    "\n",
    "                # Decode the generated subwords so far\n",
    "                subword_text = self.tokenizer.decode([next_token_id], clean_up_tokenization_spaces=True)\n",
    "                # print(\"subword\", subword_text, subword_text.lower() in english_words)\n",
    "\n",
    "                # Check if the last token can complete a word\n",
    "                if not subword_text.startswith('[unused') and subword_text != self.tokenizer.pad_token:\n",
    "                    if subword == 0:\n",
    "                        i += 1\n",
    "                    # is the word complete?\n",
    "                    if subword_text.lower() in english_words and len(generated_subwords) == 0:\n",
    "                        suggestions.append(subword_text)\n",
    "                        break\n",
    "                    # Check if it's not a continuation of a word\n",
    "                    if not subword_text.startswith(\"##\") and len(generated_subwords) > 0:\n",
    "                        break\n",
    "                    if subword_text.startswith(\"##\"):\n",
    "                        # is the word complete?\n",
    "                        if len(generated_subwords) == 0 and prefix + subword_text[2:] in english_words:\n",
    "                            if prefix + subword_text[2:] not in suggestions:\n",
    "                                suggestions.append(prefix + subword_text[2:])\n",
    "                            break\n",
    "                        else:\n",
    "                            if len(generated_subwords) > 0:\n",
    "                                if generated_subwords[-1] + subword_text[2:] in english_words:\n",
    "                                    if generated_subwords[-1] + subword_text[2:] not in suggestions:\n",
    "                                        suggestions.append(generated_subwords[-1] + subword_text[2:])\n",
    "                                    break\n",
    "                                else:\n",
    "                                    generated_subwords.append(generated_subwords[-1] + subword_text[2:])\n",
    "                                    next_token_id_input = self.tokenizer.encode(generated_subwords[-1] + subword_text[2:], add_special_tokens=False)\n",
    "                                    input_ids = torch.cat([input_ids, torch.tensor([next_token_id_input]).to(self.device)], dim=1).to(self.device)  # Append the predicted token to the input\n",
    "                            else:\n",
    "                                generated_subwords.append(prefix + subword_text[2:])\n",
    "                                next_token_id_input = self.tokenizer.encode(prefix + subword_text[2:], add_special_tokens=False)\n",
    "                                input_ids = torch.cat([input_ids, torch.tensor([next_token_id_input]).to(self.device)], dim=1).to(self.device)  # Append the predicted token to the input\n",
    "            input_ids = input_ids_start\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f888a2-ebb2-48db-9ec8-d88d7e954000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 10:12:56.774088: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-04 10:12:56.804076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-04 10:12:56.804117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-04 10:12:56.804937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-04 10:12:56.809996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-5 word-level accuracy of the model with artificial padding is  0.245\n",
      "The top-1 word-level accuracy of the model with artificial padding is  0.09\n",
      "Running on cuda\n",
      "The top-5 word-level accuracy of the model without artificial padding is  0.22\n",
      "The top-1 word-level accuracy of the model without artificial padding is  0.1\n"
     ]
    }
   ],
   "source": [
    "## Test padding and no padding models on sample mobile sentences\n",
    "\n",
    "def experiment1(with_padding):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Reproducibility\n",
    "    np.random.seed(5719)\n",
    "    \n",
    "    device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "    print( \"Running on\", device )\n",
    "    \n",
    "    if with_padding:\n",
    "        rnn_model = torch.load('Final_with_padding/rnn.model')\n",
    "    else:\n",
    "        rnn_model = torch.load('Final_no_padding/rnn.model')\n",
    "    rnn = RNNpredictor(rnn_model, tokenizer, device)\n",
    "    \n",
    "    \n",
    "    rnn_model.eval()\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    correct_5, incorrect_5 = 0,0\n",
    "    correct_1, incorrect_1 = 0,0\n",
    "    with open('samples_mobile_test.txt') as f:\n",
    "        for line in f:\n",
    "            sentence = line.strip().split()\n",
    "            sequence = ' '.join(sentence[:-1])\n",
    "            sequence += ' '\n",
    "            target = sentence[-1]\n",
    "            next_words = rnn.predict_next_word(sequence, 5)\n",
    "            if target in next_words:\n",
    "                correct_5 += 1\n",
    "            else:\n",
    "                incorrect_5 += 1\n",
    "            if target == next_words[0]:\n",
    "                correct_1 += 1\n",
    "            else: \n",
    "                incorrect_1 += 1\n",
    "    word_accuracy_5 = correct_5 / (correct_5 + incorrect_5)\n",
    "    if with_padding:\n",
    "        print(\"The top-5 word-level accuracy of the model with artificial padding is \", word_accuracy_5)\n",
    "    else: \n",
    "        print(\"The top-5 word-level accuracy of the model without artificial padding is \", word_accuracy_5)\n",
    "    \n",
    "    word_accuracy_1 = correct_1 / (correct_1 + incorrect_1)\n",
    "    if with_padding:\n",
    "        print(\"The top-1 word-level accuracy of the model with artificial padding is \", word_accuracy_1)\n",
    "    else:\n",
    "        print(\"The top-1 word-level accuracy of the model without artificial padding is \", word_accuracy_1)\n",
    "\n",
    "\n",
    "experiment1(True)\n",
    "experiment1(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ceb8ba-d21f-47e7-9fd4-416dbe56ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Experiment 2  ==================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb29976-67ac-4df5-8f7e-33d214a537e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device, criterion):\n",
    "    model.eval()\n",
    "    correct, incorrect, total_loss = 0, 0, 0\n",
    "    hidden = None\n",
    "    for seq, label in dataloader:\n",
    "        sequence, label = seq.to(device), label.to(device)\n",
    "        logits, _ = model(sequence, hidden)\n",
    "        _, predicted_word_ids = logits.topk(1)\n",
    "        assert (label.shape == predicted_word_ids.squeeze(1).shape)\n",
    "        total_loss += criterion(logits.squeeze(1), label).item()\n",
    "        comparison = torch.eq(label, predicted_word_ids.squeeze(1))\n",
    "        count_same_entries = torch.sum(comparison).item()\n",
    "        #count_same_entries = (label == predicted_word_ids.squeeze(1)).sum().item()\n",
    "        \n",
    "        correct += count_same_entries\n",
    "        incorrect += label.shape[0] - count_same_entries\n",
    "\n",
    "    print( \"Correctly predicted words    : \", correct )\n",
    "    print( \"Incorrectly predicted words  : \", incorrect )\n",
    "    print( \"Accuracy                     : \", correct / (correct + incorrect))\n",
    "    print(\"PPL                           : \", math.exp(total_loss/len(dataloader))\n",
    "    \n",
    "    return correct / (correct + incorrect), math.exp(total_loss/len(dataloader))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "954a8183-fe1a-44fb-91d6-2661e449c834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Read in  data/clean_data/twitter.txt\n",
      "Finished reading file: data/clean_data/twitter.txt\n",
      "Time taken to read data/clean_data/twitter.txt: 451.49 seconds\n",
      "Evaluating on the test data...\n",
      "Number of test sentences:  256621\n",
      "\n",
      "Correctly predicted words    :  1979975\n",
      "Incorrectly predicted words  :  14443769\n",
      "Accuracy                     :  0.12055564188043846\n",
      "PPL                           :  1.1110886593104148\n",
      "Running on cuda\n",
      "Read in  data/clean_data/news_summarization.txt\n",
      "Finished reading file: data/clean_data/news_summarization.txt\n",
      "Time taken to read data/clean_data/news_summarization.txt: 189.64 seconds\n",
      "Evaluating on the test data...\n",
      "Number of test sentences:  116749\n",
      "\n",
      "Correctly predicted words    :  1124400\n",
      "Incorrectly predicted words  :  6347536\n",
      "Accuracy                     :  0.15048308764957302\n",
      "PPL                           :  1.1017415763762994\n"
     ]
    }
   ],
   "source": [
    "def experiment2evaluation(news, twitter, articles, with_padding):\n",
    "    # ==================== Model Setup ==================== #\n",
    "    batch_size = 64\n",
    "    embedding_size = 50 #16\n",
    "    hidden_size = 64 #25\n",
    "    num_layers = 2\n",
    "    seq_length = 5      # number of tokens used as a datapoint\n",
    "    learning_rate = 0.001\n",
    "    epochs = 1\n",
    "    num_processes = 4\n",
    "    use_GRU = True\n",
    "    dropout = 0.5\n",
    "    artificial_padding = True\n",
    "    \n",
    "    \n",
    "    device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "    print( \"Running on\", device )\n",
    "\n",
    "    if with_padding:\n",
    "        rnn_model = torch.load('Final_with_padding/rnn.model')\n",
    "    else:\n",
    "        rnn_model = torch.load('Final_no_padding/rnn.model')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # ==================== Data Setup ==================== #\n",
    "    filenames = []\n",
    "    \n",
    "    if news:\n",
    "        filenames.append('data/clean_data/news_summarization.txt')\n",
    "    if twitter: \n",
    "        filenames.append('data/clean_data/twitter.txt')\n",
    "    if articles:\n",
    "        filenames.append('data/clean_data/articles.txt')\n",
    "    \n",
    "    # choose tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Reproducibility\n",
    "    np.random.seed(5719)\n",
    "\n",
    "    # set up dataloaders\n",
    "    dataset = WPDataset(filenames=filenames, tokenizer=tokenizer, samples_length=seq_length, artificial_padding=artificial_padding)\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    training_data, validation_data, test_data = random_split(dataset, [0.8, 0.05, 0.15], generator=generator)\n",
    "    \n",
    "    val_dataloader = DataLoader(validation_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, drop_last=True, num_workers=4, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # ==================== Evaluation ==================== #\n",
    "    rnn_model.eval()\n",
    "    print( \"Evaluating on the test data...\" )\n",
    "    \n",
    "    print( \"Number of test sentences: \", len(test_dataloader) )\n",
    "    print()\n",
    "    \n",
    "    test_accuracy, ppl = evaluate(test_dataloader, rnn_model, device, criterion)\n",
    "\n",
    "news = False\n",
    "twitter = True\n",
    "articles = False\n",
    "with_padding = True\n",
    "experiment2evaluation(news, twitter, articles, with_padding)\n",
    "\n",
    "news = True\n",
    "twitter = False\n",
    "articles = False\n",
    "with_padding = True\n",
    "experiment2evaluation(news, twitter, articles, with_padding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
