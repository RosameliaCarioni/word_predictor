{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e404c1d-547f-47f5-b7e0-a0d322914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from data_loading_utils import read_lines_from_file_as_data_chunks\n",
    "import time  # Import the time module\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ab477-3aa2-4f09-8009-9f997c6e246b",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2c074-9273-4a00-8857-741da5fd9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading clean text from txt files to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    Datapoints are sequences of words (tokenized) + label (next token). If the \n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    chunk_size: how much we read from the file at the time - we could play around with it. \n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, tokenizer, samples_length=5, chunk_size=1000000, artificial_padding=True):\n",
    "        self.sequences = [] # X\n",
    "        self.labels = [] # Y \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples_length = samples_length\n",
    "        self.artificial_padding = artificial_padding\n",
    "        self.pad_token_id = tokenizer.pad_token_id  # Get the PAD token ID = 0 \n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.read_file, filename, chunk_size) for filename in filenames]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure all files are processed\n",
    "\n",
    "    def read_file(self, filename, chunk_size):\n",
    "        print(\"Read in \", filename)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            read_lines_from_file_as_data_chunks(filename, chunk_size, self.process_lines)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        end_time = time.time()  # End the timer\n",
    "        print(f\"Time taken to read {filename}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def process_lines(self, data, eof, file_name):\n",
    "        \"\"\"\n",
    "        eof: end of file \n",
    "        Callback function to process lines read from file.\n",
    "        \"\"\"\n",
    "        if not eof:\n",
    "            text = data.strip()  # Remove leading/trailing whitespace\n",
    "            # split sentence into sub-sentences so that it can be passed to tokenizer, which has a max capacity of 512 \n",
    "            line_chunks = self.split_into_chunks(text) \n",
    "            for chunk in line_chunks:\n",
    "                line_tokens = self.tokenizer.tokenize(chunk) # data is already lower case \n",
    "                line_tokens_ids = self.tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "                self.create_sequences(line_tokens_ids)\n",
    "        else:\n",
    "            print(f\"Finished reading file: {file_name}\")\n",
    "\n",
    "    def split_into_chunks(self, line, max_length=512):\n",
    "        \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "        return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "    def create_sequences(self, token_ids):\n",
    "        \"\"\"\n",
    "        Create sequences and labels from tokenized text.\n",
    "        \"\"\"\n",
    "        n = self.samples_length\n",
    "        if self.artificial_padding:\n",
    "            k = 0 \n",
    "            while k < len(token_ids) - n:\n",
    "                for i in range(1, n + 1):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)\n",
    "                k += n\n",
    "            remaining_tokens = len(token_ids) - k\n",
    "            if remaining_tokens > 1:\n",
    "                for i in range(1, remaining_tokens):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)     \n",
    "        else: \n",
    "            # Ensure all sequences are of length samples_length\n",
    "            for i in range(self.samples_length, len(token_ids)): # sliding window \n",
    "                seq = token_ids[i-self.samples_length:i]\n",
    "                label = token_ids[i]\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ac934-1148-41e5-be42-0dab283280c4",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7ad76-544b-4415-b9fb-01fdd736aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['data/clean_data/news_summarization.txt', 'data/clean_data/twitter.txt']\n",
    "\n",
    "# Define the tokenizer (using BERT tokenizer as an example)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = WPDataset(filenames, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821d58c-3758-44c1-ad28-4fe2a90c2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0079f5-1260-449e-820e-6e87b4bf9eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2821, 2129, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b38681b-e155-42a3-9d11-157361380fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##ed'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68de5b-9db7-485f-a56f-1b14a6a2954d",
   "metadata": {},
   "source": [
    "# Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df229caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1822db8-064d-4eab-acf1-17d762754a4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through the DataLoader\n",
    "i = 0 \n",
    "for batch in dataloader:\n",
    "    sequences, labels = batch\n",
    "    print(sequences.shape, labels.shape)\n",
    "    print(sequences)\n",
    "    print(labels)\n",
    "    print('')     \n",
    "    # Your training loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b32012-d4d2-4b8e-92de-b1a951234331",
   "metadata": {},
   "source": [
    "# Testing loading configs/experiments:\n",
    "Total data: 5.862,7 MB\n",
    "\n",
    "news_summarization.txt: 264MB \n",
    "twitter.txt: 551,9 MB\n",
    "\n",
    "USING news_summarization.txt ONLY \n",
    "\n",
    "1. chunk_size=1000000, artifical_padding = False   \n",
    "   time = 528.20 seconds, memory = 6.44 GB\n",
    "2. chunk_size=1000000, artifical_padding = True\n",
    "   time = 571.33 seconds, memory = 6.93 GB \n",
    "\n",
    "3. chunk_size=2000000, artifical_padding = True\n",
    "   time = 561.67 seconds, memory = 6.95 GB\n",
    "4. chunk_size=500000, artifical_padding = True\n",
    "   time = 562.89 seconds, memory = 6.95 GB \n",
    "   \n",
    "5. Thread, chunk_size=1000000, artifical_padding = True\n",
    "   time = 546.93 seconds, memory = 6.86 GB\n",
    "\n",
    "USING news_summarization.txt AND twitter.txt  \n",
    "\n",
    "6. Thread, chunk_size=1000000, artifical_padding = True\n",
    "   time = 1102.74 seconds, memory =  13.61 GB\n",
    "\n",
    "7. No tread, chunk_size=1000000, artifical_padding = True\n",
    "    time = 515.84 + 1158.22 , memory = 20.01 GB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052e809",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
