{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e404c1d-547f-47f5-b7e0-a0d322914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in /opt/conda/lib/python3.10/site-packages (1.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec69c5ac-b35c-4c32-bf2c-80d2937117f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizer\n",
    "from data_loading_utils import read_lines_from_file_as_data_chunks\n",
    "import time  # Import the time module\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ab477-3aa2-4f09-8009-9f997c6e246b",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff2c074-9273-4a00-8857-741da5fd9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading clean text from txt files to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    Datapoints are sequences of words (tokenized) + label (next token). If the \n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    chunk_size: how much we read from the file at the time - we could play around with it. \n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, tokenizer, samples_length=5, chunk_size=1000000, artificial_padding=True):\n",
    "        self.sequences = [] # X\n",
    "        self.labels = [] # Y \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples_length = samples_length\n",
    "        self.artificial_padding = artificial_padding\n",
    "        self.pad_token_id = tokenizer.pad_token_id  # Get the PAD token ID = 0 \n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.read_file, filename, chunk_size) for filename in filenames]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure all files are processed\n",
    "\n",
    "    def read_file(self, filename, chunk_size):\n",
    "        print(\"Read in \", filename)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            read_lines_from_file_as_data_chunks(filename, chunk_size, self.process_lines)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        end_time = time.time()  # End the timer\n",
    "        print(f\"Time taken to read {filename}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def process_lines(self, data, eof, file_name):\n",
    "        \"\"\"\n",
    "        eof: end of file \n",
    "        Callback function to process lines read from file.\n",
    "        \"\"\"\n",
    "        if not eof:\n",
    "            text = data.strip()  # Remove leading/trailing whitespace\n",
    "            # split sentence into sub-sentences so that it can be passed to tokenizer, which has a max capacity of 512 \n",
    "            line_chunks = self.split_into_chunks(text) \n",
    "            for chunk in line_chunks:\n",
    "                line_tokens = self.tokenizer.tokenize(chunk) # data is already lower case \n",
    "                line_tokens_ids = self.tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "                self.create_sequences(line_tokens_ids)\n",
    "        else:\n",
    "            print(f\"Finished reading file: {file_name}\")\n",
    "\n",
    "    def split_into_chunks(self, line, max_length=512):\n",
    "        \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "        return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "    def create_sequences(self, token_ids):\n",
    "        \"\"\"\n",
    "        Create sequences and labels from tokenized text.\n",
    "        \"\"\"\n",
    "        n = self.samples_length\n",
    "        if self.artificial_padding:\n",
    "            k = 0 \n",
    "            while k < len(token_ids) - n:\n",
    "                for i in range(1, n + 1):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)\n",
    "                k += n\n",
    "            remaining_tokens = len(token_ids) - k\n",
    "            if remaining_tokens > 1:\n",
    "                for i in range(1, remaining_tokens):\n",
    "                    seq = token_ids[k:i+k] + [self.pad_token_id] * (n - i)\n",
    "                    label = token_ids[i + k]\n",
    "                    self.sequences.append(seq)\n",
    "                    self.labels.append(label)     \n",
    "        else: \n",
    "            # Ensure all sequences are of length samples_length\n",
    "            for i in range(self.samples_length, len(token_ids)): # sliding window \n",
    "                seq = token_ids[i-self.samples_length:i]\n",
    "                label = token_ids[i]\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ac934-1148-41e5-be42-0dab283280c4",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f7ad76-544b-4415-b9fb-01fdd736aa65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in  data/articles.txt\n",
      "Finished reading file: data/articles.txt\n",
      "Time taken to read data/articles.txt: 6.99 seconds\n"
     ]
    }
   ],
   "source": [
    "filenames = ['data/articles.txt'] #,'data/clean_data/news_summarization.txt', 'data/clean_data/twitter.txt', 'data/clean_data/mobile_text.txt']\n",
    "\n",
    "# Define the tokenizer (using BERT tokenizer as an example)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = WPDataset(filenames, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b821d58c-3758-44c1-ad28-4fe2a90c2d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699865"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d0079f5-1260-449e-820e-6e87b4bf9eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2821, 0, 0, 0, 0]   2129\n",
      "\n",
      "[2821, 2129, 0, 0, 0]   1996\n",
      "\n",
      "[2821, 2129, 1996, 0, 0]   19377\n",
      "\n",
      "[2821, 2129, 1996, 19377, 0]   1038\n",
      "\n",
      "[2821, 2129, 1996, 19377, 1038]   8017\n",
      "\n",
      "[8017, 0, 0, 0, 0]   2098\n",
      "\n",
      "[8017, 2098, 0, 0, 0]   11834\n",
      "\n",
      "[8017, 2098, 11834, 0, 0]   27014\n",
      "\n",
      "[8017, 2098, 11834, 27014, 0]   2020\n",
      "\n",
      "[8017, 2098, 11834, 27014, 2020]   1996\n",
      "\n",
      "[1996, 0, 0, 0, 0]   2279\n",
      "\n",
      "[1996, 2279, 0, 0, 0]   2502\n",
      "\n",
      "[1996, 2279, 2502, 0, 0]   2518\n",
      "\n",
      "[1996, 2279, 2502, 2518, 0]   2256\n",
      "\n",
      "[1996, 2279, 2502, 2518, 2256]   8069\n",
      "\n",
      "[8069, 0, 0, 0, 0]   2020\n",
      "\n",
      "[8069, 2020, 0, 0, 0]   3712\n",
      "\n",
      "[8069, 2020, 3712, 0, 0]   2152\n",
      "\n",
      "[8069, 2020, 3712, 2152, 0]   4408\n",
      "\n",
      "[8069, 2020, 3712, 2152, 4408]   7168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    print(dataset.sequences[i], ' ', dataset.labels[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b38681b-e155-42a3-9d11-157361380fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec137be-832f-4864-84f9-6cd7e263870d",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea88e9-58f3-4237-bc0b-f324c6540a37",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d3d524-a6c4-4059-8e65-0b15ad77b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3be1b40-3a43-4baa-8871-e04ab5dcaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.1, last_output=True, max_len=5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.last_output = last_output\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        if last_output:\n",
    "            self.linear = nn.Linear(d_model, ntoken)\n",
    "        else:\n",
    "            self.linear = nn.Linear(d_model*max_len, ntoken)\n",
    "        \n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[batch_size, seq_len]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[batch_size, seq_len, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        if self.last_output:\n",
    "            output = self.linear(output[:, -1, :])  # Take the last token's output\n",
    "            return output\n",
    "        else:\n",
    "            flattened_transf = output.reshape(src.size(0), 1, -1)  # Flatten the output\n",
    "            result = self.linear(torch.tanh(flattened_transf))\n",
    "            return result.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4cdc2-9001-4a25-8122-034776b1ade2",
   "metadata": {},
   "source": [
    "# Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f85d5a04-be24-4059-a622-538bca2e870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = 37000\n",
    "d_model = 512\n",
    "num_layers = 6 # 6\n",
    "dim_hid = 2048\n",
    "seq_length = 5\n",
    "expansion_factor = 4\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerModel(ntoken=ntoken, d_model=d_model, nhead=n_heads, nlayers=num_layers, d_hid=dim_hid, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8af1ed3-6f35-48c8-b41e-7a0f64eb982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "ntoken = 30522  # Vocabulary size\n",
    "d_model = 768  # Model dimension\n",
    "nhead = 12  # Number of attention heads\n",
    "d_hid = 3072  # Hidden dimension in the feedforward layer\n",
    "nlayers = 3  # Number of transformer layers\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "model = TransformerModel(ntoken, d_model, nhead, d_hid, nlayers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0472d54f-92df-4bcf-a2d4-88d623009dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─PositionalEncoding: 1-1                     --\n",
      "|    └─Dropout: 2-1                           --\n",
      "├─TransformerEncoder: 1-2                     --\n",
      "|    └─ModuleList: 2-2                        --\n",
      "|    |    └─TransformerEncoderLayer: 3-1      7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-2      7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-3      7,087,872\n",
      "├─Embedding: 1-3                              23,440,896\n",
      "├─Linear: 1-4                                 23,471,418\n",
      "======================================================================\n",
      "Total params: 68,175,930\n",
      "Trainable params: 68,175,930\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─PositionalEncoding: 1-1                     --\n",
       "|    └─Dropout: 2-1                           --\n",
       "├─TransformerEncoder: 1-2                     --\n",
       "|    └─ModuleList: 2-2                        --\n",
       "|    |    └─TransformerEncoderLayer: 3-1      7,087,872\n",
       "|    |    └─TransformerEncoderLayer: 3-2      7,087,872\n",
       "|    |    └─TransformerEncoderLayer: 3-3      7,087,872\n",
       "├─Embedding: 1-3                              23,440,896\n",
       "├─Linear: 1-4                                 23,471,418\n",
       "======================================================================\n",
       "Total params: 68,175,930\n",
       "Trainable params: 68,175,930\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68de5b-9db7-485f-a56f-1b14a6a2954d",
   "metadata": {},
   "source": [
    "# Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b7f7631-63ab-45ca-bcb2-aa507927b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "n_epoch = 5\n",
    "\n",
    "ntoken = 30522  # Vocabulary size\n",
    "d_model = 768  # Model dimension\n",
    "nhead = 12  # Number of attention heads\n",
    "d_hid = 3072  # Hidden dimension in the feedforward layer\n",
    "nlayers = 3  # Number of transformer layers - use 3 or 6\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "model = TransformerModel(ntoken=ntoken, d_model=d_model, nhead=nhead, nlayers=nlayers, \n",
    "                         d_hid=d_hid, dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78b67f80-ca1d-4d05-8eb5-ee2bcf0f1f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in  data/articles.txt\n",
      "Finished reading file: data/articles.txt\n",
      "Time taken to read data/articles.txt: 6.61 seconds\n"
     ]
    }
   ],
   "source": [
    "filenames = ['data/articles.txt'] #,'data/clean_data/news_summarization.txt', 'data/clean_data/twitter.txt', 'data/clean_data/mobile_text.txt']\n",
    "\n",
    "# Define the tokenizer (using BERT tokenizer as an example)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = WPDataset(filenames, tokenizer)\n",
    "\n",
    "# Define the split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "912febdf-038e-42f6-a806-41ec253a99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device):\n",
    "    correct, incorrect = 0, 0\n",
    "    model.eval()\n",
    "    for seq, label in dataloader:\n",
    "        sequence, label = seq.to(device), label.to(device)\n",
    "        logits = model(sequence).to(device)\n",
    "        _, predicted_word_ids = logits.topk(1)\n",
    "        assert (label.shape == predicted_word_ids.squeeze(1).shape)\n",
    "        comparison = torch.eq(label, predicted_word_ids.squeeze(1))\n",
    "        count_same_entries = torch.sum(comparison).item()\n",
    "        count_same_entries = (label == predicted_word_ids.squeeze(1)).sum().item()\n",
    "        \n",
    "        correct += count_same_entries\n",
    "        incorrect += label.shape[0] - count_same_entries\n",
    "\n",
    "    print(\"Correctly predicted words    :\", correct)\n",
    "    print(\"Incorrectly predicted words  :\", incorrect)\n",
    "    return correct/(correct+incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81dff6f3-159e-4322-847c-d05722deb084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:50:24 Training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 8749/8749 [06:55<00:00, 21.06it/s, loss=7.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 7.184396520316608\n",
      "Evaluating on the validation data...\n",
      "Correctly predicted words    : 6293\n",
      "Incorrectly predicted words  : 133680\n",
      "Validation accuracy: 0.04495867060075872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 8749/8749 [07:02<00:00, 20.71it/s, loss=7.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 7.078634879700537\n",
      "Evaluating on the validation data...\n",
      "Correctly predicted words    : 6293\n",
      "Incorrectly predicted words  : 133680\n",
      "Validation accuracy: 0.04495867060075872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 8749/8749 [06:51<00:00, 21.25it/s, loss=7.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 7.068792683449537\n",
      "Evaluating on the validation data...\n",
      "Correctly predicted words    : 6293\n",
      "Incorrectly predicted words  : 133680\n",
      "Validation accuracy: 0.04495867060075872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 8749/8749 [06:49<00:00, 21.39it/s, loss=7.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 7.0646172062521435\n",
      "Evaluating on the validation data...\n",
      "Correctly predicted words    : 6293\n",
      "Incorrectly predicted words  : 133680\n",
      "Validation accuracy: 0.04495867060075872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 8749/8749 [06:48<00:00, 21.44it/s, loss=7.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 7.062300423619842\n",
      "Evaluating on the validation data...\n",
      "Correctly predicted words    : 6293\n",
      "Incorrectly predicted words  : 133680\n",
      "Validation accuracy: 0.04495867060075872\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "model.train()  # turn on train mode\n",
    "print(datetime.now().strftime(\"%X\"), \"Training starts\")\n",
    "for epoch in range(n_epoch):\n",
    "    iteration = 0\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, desc=\"Epoch {}\".format(epoch + 1)) as tepoch:\n",
    "        for sequence, label in tepoch:\n",
    "            sequence, label = sequence.to(device), label.to(device)\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(sequence).to(device)        \n",
    "            loss = criterion(logits.squeeze(1), label)\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            iteration += 1\n",
    "            total_loss += loss \n",
    "            # Update tqdm description with the current loss\n",
    "            tepoch.set_postfix(loss=(total_loss / iteration).item())\n",
    "        \n",
    "    print(\"Epoch\", epoch+1, \"loss:\", total_loss.detach().item()/iteration)\n",
    "    \n",
    "    print(\"Evaluating on the validation data...\")\n",
    "    word_level_accuracy = evaluate(val_loader, model, device)\n",
    "    print(\"Validation accuracy:\", word_level_accuracy)\n",
    "    # save best model\n",
    "    if word_level_accuracy > best_val_accuracy:\n",
    "        torch.save(model.state_dict(), 'weights/tranformer6.pt')\n",
    "        best_val_accuracy = word_level_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37b623e9-7dab-4816-ba2e-6d483f0271f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n",
      "Correctly predicted words    : 0\n",
      "Incorrectly predicted words  : 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_model \u001b[38;5;241m=\u001b[39m TransformerModel(ntoken\u001b[38;5;241m=\u001b[39mntoken, d_model\u001b[38;5;241m=\u001b[39md_model, nhead\u001b[38;5;241m=\u001b[39mnhead, nlayers\u001b[38;5;241m=\u001b[39mnlayers, \n\u001b[1;32m      2\u001b[0m                          d_hid\u001b[38;5;241m=\u001b[39md_hid, dropout\u001b[38;5;241m=\u001b[39mdropout)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m test_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/tranformer.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, model, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrectly predicted words    :\u001b[39m\u001b[38;5;124m\"\u001b[39m, correct)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrectly predicted words  :\u001b[39m\u001b[38;5;124m\"\u001b[39m, incorrect)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mincorrect\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "test_model = TransformerModel(ntoken=ntoken, d_model=d_model, nhead=nhead, nlayers=nlayers, \n",
    "                              d_hid=d_hid, dropout=dropout).to(device)\n",
    "test_model.load_state_dict(torch.load('weights/tranformer.pt'))\n",
    "evaluate(val_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573b70d-f352-4239-aede-611c6ac9127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(model: nn.Module, n_epoch, training_loader) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "    for epoch in range(n_epoch):\n",
    "    iteration = 0\n",
    "    for input_tensor, label in training_loader:\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = model(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "        iteration += 1\n",
    "\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990f9b6-1a53-4b11-a020-cdae1b37032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ef73b9e-d018-47d2-8a6a-8be9aa752327",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = TransformerModel(d_model=d_model, src_vocab_size=src_vocab_size, target_vocab_size=target_vocab_size, seq_length=seq_length,num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads, dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc50b130-f669-4716-923f-e171442aa11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:51:19 Training starts\n",
      "src shape:  torch.Size([64, 5])\n",
      "trg shape:  torch.Size([64])\n",
      "pe_expanded:  torch.Size([64, 5, 512])\n",
      "x:  torch.Size([64, 5, 512])\n",
      "\n",
      "pe_expanded:  torch.Size([64, 5, 512])\n",
      "x:  torch.Size([64, 512])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m model_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Shift the label to create the target sequence\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#trg_input = torch.cat([torch.full((label.size(0), 1), start_token, dtype=torch.long).to(device), label[:, :-1]], dim=1)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), label)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[53], line 62\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     60\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#trg_mask = self.make_target_mask(trg)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# trg_mask)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 34\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, enc_out)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    x: input vector from target\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    out: output vector\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(x) \n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder(tgt\u001b[38;5;241m=\u001b[39mx, memory\u001b[38;5;241m=\u001b[39menc_out)\u001b[38;5;66;03m#,tgt_mask=target_mask)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#output = F.softmax(self.linear(x), dim=-1)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 34\u001b[0m, in \u001b[0;36mPositionalEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx: \u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpe_expanded\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print(datetime.now().strftime(\"%X\"), \"Training starts\")\n",
    "for epoch in range(n_epoch) :\n",
    "    iteration = 0\n",
    "    for input_tensor, label in training_loader :\n",
    "        sequence, label = input_tensor.to(device), label.to(device)\n",
    "        model_optimizer.zero_grad()\n",
    "        \n",
    "        # Shift the label to create the target sequence\n",
    "        #trg_input = torch.cat([torch.full((label.size(0), 1), start_token, dtype=torch.long).to(device), label[:, :-1]], dim=1)\n",
    "\n",
    "        logits = model(sequence, label).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        iteration += 1\n",
    "    print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01b5e8-ee7c-46c2-97f4-2edec96210a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    charlm.eval()\n",
    "    # Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        char_list = list(\"he took out his wand and\"[-MAXLEN:])\n",
    "        for i in range(300) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue\n",
    "    charlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1822db8-064d-4eab-acf1-17d762754a4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate through the DataLoader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m      5\u001b[0m         sequences, labels \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[11], line 69\u001b[0m, in \u001b[0;36mWPDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx])\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "i = 0 \n",
    "for batch in dataloader:\n",
    "    sequences, labels = batch\n",
    "    print(sequences.shape, labels.shape)\n",
    "    print(sequences)\n",
    "    print(labels)\n",
    "    print('')     \n",
    "    # Your training loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b32012-d4d2-4b8e-92de-b1a951234331",
   "metadata": {},
   "source": [
    "# Testing loading configs:\n",
    "Total data: 5.862,7 MB\n",
    "\n",
    "news_summarization.txt: 264MB \n",
    "twitter.txt: 551,9 GB\n",
    "\n",
    "USING news_summarization.txt ONLY \n",
    "\n",
    "1. chunk_size=1000000, artifical_padding = False   \n",
    "   time = 528.20 seconds, memory = 6.44 GB\n",
    "2. chunk_size=1000000, artifical_padding = True\n",
    "   time = 571.33 seconds, memory = 6.93 GB \n",
    "\n",
    "3. chunk_size=2000000, artifical_padding = True\n",
    "   time = 561.67 seconds, memory = 6.95 GB\n",
    "4. chunk_size=500000, artifical_padding = True\n",
    "   time = 562.89 seconds, memory = 6.95 GB \n",
    "   \n",
    "5. Thread, chunk_size=1000000, artifical_padding = True\n",
    "   time = 546.93 seconds, memory = 6.86 GB\n",
    "\n",
    "USING news_summarization.txt AND twitter.txt  \n",
    "\n",
    "6. Thread, chunk_size=1000000, artifical_padding = True\n",
    "   time = 1102.74 seconds, memory =  13.61 GB\n",
    "\n",
    "7. No tread, chunk_size=1000000, artifical_padding = True\n",
    "    time = 515.84 + 1158.22 , memory = 20.01 GB "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
