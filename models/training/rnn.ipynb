{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e404c1d-547f-47f5-b7e0-a0d322914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kathideckenbach/anaconda3/envs/word_predictor/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63344ad-1350-4d20-a2a4-94cabd26edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(line):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(line):\n",
    "        end = min(start + 512, len(line))\n",
    "        chunks.append(line[start:end])\n",
    "        start = end\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b003d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def split_into_chunks(line, max_length=512):\n",
    "    \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "    return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "def process_line(line, tokenizer, n):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    line = line.strip()\n",
    "    line_chunks = split_into_chunks(line)\n",
    "    for chunk in line_chunks:\n",
    "        line_tokens = tokenizer.tokenize(chunk)\n",
    "        line_tokens = tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "        k = 0\n",
    "        while k < len(line_tokens) - n:\n",
    "            for i in range(1, n + 1):\n",
    "                sequences.append([c for c in line_tokens[k:i + k] + [0] * (n - i)])\n",
    "                labels.append(line_tokens[i + k])\n",
    "            k += n\n",
    "        remaining_tokens = len(line_tokens) - k\n",
    "        if remaining_tokens > 1:\n",
    "            for i in range(1, remaining_tokens):\n",
    "                sequences.append([c for c in line_tokens[k:i + k] + [0] * (n - i)])\n",
    "                labels.append(line_tokens[i + k])\n",
    "    return sequences, labels\n",
    "\n",
    "class WPDataset(Dataset):\n",
    "    def __init__(self, filenames, tokenizer, n):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        pool = Pool(cpu_count())\n",
    "\n",
    "        for filename in filenames:\n",
    "            print(\"Read in \", filename)\n",
    "            try:\n",
    "                with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    \n",
    "                # Use multiprocessing to process lines in parallel\n",
    "                results = []\n",
    "                for line in tqdm(lines, desc=\"Processing lines\"):\n",
    "                    result = pool.apply_async(process_line, (line, tokenizer, n))\n",
    "                    results.append(result)\n",
    "\n",
    "                # Collect results\n",
    "                for result in tqdm(results, desc=\"Collecting results\"):\n",
    "                    seq, lbl = result.get()\n",
    "                    self.sequences.extend(seq)\n",
    "                    self.labels.extend(lbl)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09c8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(line, max_length=512):\n",
    "    \"\"\"Splits a long line into chunks of max_length tokens.\"\"\"\n",
    "    return [line[i:i + max_length] for i in range(0, len(line), max_length)]\n",
    "\n",
    "def process_line(args):\n",
    "    line, tokenizer, n = args\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    line = line.strip()\n",
    "    line_chunks = split_into_chunks(line)\n",
    "    for chunk in line_chunks:\n",
    "        line_tokens = tokenizer.tokenize(chunk)\n",
    "        line_tokens = tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "        k = 0\n",
    "        while k < len(line_tokens) - n:\n",
    "            for i in range(1, n + 1):\n",
    "                sequences.append(line_tokens[k:i + k] + [0] * (n - i))\n",
    "                labels.append(line_tokens[i + k])\n",
    "            k += n\n",
    "        remaining_tokens = len(line_tokens) - k\n",
    "        if remaining_tokens > 1:\n",
    "            for i in range(1, remaining_tokens):\n",
    "                sequences.append(line_tokens[k:i + k] + [0] * (n - i))\n",
    "                labels.append(line_tokens[i + k])\n",
    "    return sequences, labels\n",
    "\n",
    "def read_file_in_chunks(filename, chunk_size=1024*1024):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        while True:\n",
    "            lines = f.readlines(chunk_size)\n",
    "            if not lines:\n",
    "                break\n",
    "            yield lines\n",
    "\n",
    "class WPDataset(Dataset):\n",
    "    def __init__(self, filenames, tokenizer, n):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n = n\n",
    "\n",
    "        all_sequences = []\n",
    "        all_labels = []\n",
    "\n",
    "        for filename in filenames:\n",
    "            print(\"Reading \", filename)\n",
    "            try:\n",
    "                for lines in read_file_in_chunks(filename):\n",
    "                    with Pool(cpu_count()) as pool:\n",
    "                        results = pool.map(process_line, [(line, tokenizer, n) for line in lines])\n",
    "\n",
    "                    for seq, lbl in results:\n",
    "                        all_sequences.extend(seq)\n",
    "                        all_labels.extend(lbl)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Convert lists to numpy arrays for faster access and better memory management\n",
    "        self.sequences = np.array(all_sequences, dtype=np.int32)\n",
    "        self.labels = np.array(all_labels, dtype=np.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f718622-7b3b-4284-a622-df9d24c60d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading clean from txt files to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    Datapoints are sequences of words (tokenized) + label (next token). If the \n",
    "    words have not been seen before (i.e, they are not found in the \n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filenames, tokenizer, n):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        for filename in filenames:\n",
    "            print(\"Read in \", filename)\n",
    "            try :\n",
    "                # Read the datafile\n",
    "                with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "                    lines = f.read().split('\\n')\n",
    "                    #for line in lines :\n",
    "                    for line in tqdm(lines, desc=\"Processing lines\"):\n",
    "                        line = line.strip()  # Remove leading/trailing whitespace\n",
    "                        # Split long lines into smaller chunks\n",
    "                        line_chunks = split_into_chunks(line)\n",
    "                        # Tokenize each chunk separately\n",
    "                        for chunk in line_chunks:\n",
    "                            line_tokens = tokenizer.tokenize(chunk)\n",
    "                            line_tokens = tokenizer.convert_tokens_to_ids(line_tokens)\n",
    "                            k = 0\n",
    "                            while k < len(line_tokens)-n:\n",
    "                                #print(k, len(line_tokens)-n, n)\n",
    "                                for i in range(1, n+1):\n",
    "                                    self.sequences.append([c for c in line_tokens[k:i+k]+[0]*(n-i)])\n",
    "                                    self.labels.append(line_tokens[i+k])\n",
    "                                k += n\n",
    "                            remaining_tokens = len(line_tokens) - k\n",
    "                            if remaining_tokens > 1:\n",
    "                                for i in range(1, remaining_tokens):\n",
    "                                    self.sequences.append([c for c in line_tokens[k:i+k]+[0]*(n-i)])\n",
    "                                    self.labels.append(line_tokens[i+k])\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "        # Convert lists to numpy arrays for faster access and better memory management\n",
    "        self.sequences = np.array(self.sequences, dtype=np.int32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int32)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311472af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f690cd-9067-426b-9805-982803c636b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of .txt file paths:\n",
      "../../data/clean_data/news_summarization.txt\n",
      "../../data/clean_data/news.txt\n",
      "../../data/clean_data/news_content.txt\n",
      "../../data/clean_data/mobile_text.txt\n",
      "../../data/clean_data/twitter.txt\n",
      "../../data/clean_data/articles.txt\n",
      "Reading  .\n",
      "An error occurred: [Errno 21] Is a directory: '.'\n",
      "Reading  .\n",
      "An error occurred: [Errno 21] Is a directory: '.'\n",
      "Reading  /\n",
      "An error occurred: [Errno 21] Is a directory: '/'\n",
      "Reading  .\n",
      "An error occurred: [Errno 21] Is a directory: '.'\n",
      "Reading  .\n",
      "An error occurred: [Errno 21] Is a directory: '.'\n",
      "Reading  /\n",
      "An error occurred: [Errno 21] Is a directory: '/'\n",
      "Reading  d\n",
      "File not found: d\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  t\n",
      "File not found: t\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  /\n",
      "An error occurred: [Errno 21] Is a directory: '/'\n",
      "Reading  c\n",
      "File not found: c\n",
      "Reading  l\n",
      "File not found: l\n",
      "Reading  e\n",
      "File not found: e\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  n\n",
      "File not found: n\n",
      "Reading  _\n",
      "File not found: _\n",
      "Reading  d\n",
      "File not found: d\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  t\n",
      "File not found: t\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  /\n",
      "An error occurred: [Errno 21] Is a directory: '/'\n",
      "Reading  n\n",
      "File not found: n\n",
      "Reading  e\n",
      "File not found: e\n",
      "Reading  w\n",
      "File not found: w\n",
      "Reading  s\n",
      "File not found: s\n",
      "Reading  _\n",
      "File not found: _\n",
      "Reading  s\n",
      "File not found: s\n",
      "Reading  u\n",
      "File not found: u\n",
      "Reading  m\n",
      "File not found: m\n",
      "Reading  m\n",
      "File not found: m\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  r\n",
      "File not found: r\n",
      "Reading  i\n",
      "File not found: i\n",
      "Reading  z\n",
      "File not found: z\n",
      "Reading  a\n",
      "File not found: a\n",
      "Reading  t\n",
      "File not found: t\n",
      "Reading  i\n",
      "File not found: i\n",
      "Reading  o\n",
      "File not found: o\n",
      "Reading  n\n",
      "File not found: n\n",
      "Reading  .\n",
      "An error occurred: [Errno 21] Is a directory: '.'\n",
      "Reading  t\n",
      "File not found: t\n",
      "Reading  x\n",
      "File not found: x\n",
      "Reading  t\n",
      "File not found: t\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[11], line 67\u001b[0m, in \u001b[0;36mWPDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "txt_files = [\"twitter.txt\", \"news.txt\", \"news_summarization.txt\"]\n",
    "txt_files = [\"HP_book_1.txt\", \"twitter.txt\"]\n",
    "seq_length = 5\n",
    "\n",
    "# Define the relative path from the .ipynb file to the directory containing the .txt files\n",
    "relative_path_to_txt_files = os.path.join('..', '..', 'data', 'clean_data')\n",
    "\n",
    "# Get a list of all .txt file paths in the directory\n",
    "txt_files = [os.path.join(relative_path_to_txt_files, f) for f in os.listdir(relative_path_to_txt_files) if f.endswith('.txt')]\n",
    "\n",
    "# Print the list of .txt file paths\n",
    "print(\"List of .txt file paths:\")\n",
    "for txt_file in txt_files:\n",
    "    print(txt_file)\n",
    "    \n",
    "# choose tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# set up dataloaders\n",
    "dataset = WPDataset(filenames=txt_files[0], tokenizer=tokenizer, n=seq_length)\n",
    "\n",
    "print(len(dataset))\n",
    "for i in range(20):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05745081-1832-4def-8c9a-d6b90862bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    There are two possible ways to write this class; either it tries to predict \n",
    "    a whole word that consists of several tokens or it only predicts the next token\n",
    "    after a fixed (or variable) amount of input tokens; \n",
    "    Another choice is whether to use a hidden state or not as an input to the forward pass\n",
    "    Or do a encoder - decoder structure?\n",
    "\n",
    "    I read somewhere that it is good to ... \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, hidden_size, no_of_output_symbols, device):\n",
    "        super().__init__()\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # initialize layers\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        #self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear( hidden_size, no_of_output_symbols )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size, max_seq_length)\n",
    "        Each inner list contains word IDs and represents one datapoint (n words).\n",
    "       \n",
    "        Returns:\n",
    "        the output from the RNN: logits for the predicted next word, hidden state\n",
    "        \"\"\"\n",
    "        #print(x, len(x), len(x[0]))\n",
    "        #print(torch.tensor(x))\n",
    "        #x_tensor = torch.tensor(x).to(self.device)\n",
    "        x_emb = self.embedding(x) # x_emb shape: (batch_size, max_seq_length, emb_dim)\n",
    "        output, hidden = self.rnn(x_emb, hidden) # output shape: (batch_size, max_seq_length, hidden)\n",
    "        \n",
    "        return self.output(output[:, -1, :]), hidden # logit shape: (batch_size, 1, vocab_size)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec608b05-644d-44cb-8764-8e112736941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def pad_sequence(batch, pad_symbol): #=tokenizer.pad_token):\n",
    "    \"\"\"\n",
    "    Applies padding if the number of tokens in sequences differs within one batch.\n",
    "    Only applies padding to the sequence, not the label.\n",
    "    \"\"\"\n",
    "    seq, label = zip(*batch)\n",
    "    max_seq_len = max(map(len, seq))\n",
    "    max_label_len = max(map(len, label))\n",
    "    padded_seq = [[b[i] if i < len(b) else pad_symbol for i in range(max_seq_len)] for b in seq]\n",
    "    padded_label = [[l[i] if i < len(l) else pad_symbol for i in range(max_label_len)] for l in label]\n",
    "    return padded_seq, padded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be662011-fee9-464d-93eb-f51e87e2ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, rnn_model, device):\n",
    "    correct, incorrect = 0,0\n",
    "    hidden = None\n",
    "    for seq, label in dataloader:\n",
    "        sequence, label = seq.to(device), label.to(device)\n",
    "        prediction, _ = rnn_model(sequence, hidden)\n",
    "        _, predicted_tensor = prediction.topk(1)\n",
    "\n",
    "        \n",
    "        assert (label.shape == predicted_tensor.squeeze(1).shape)\n",
    "        comparison = torch.eq(label, predicted_tensor.squeeze(1))\n",
    "        count_same_entries = torch.sum(comparison).item()\n",
    "        count_same_entries = (label == predicted_tensor.squeeze(1)).sum().item()\n",
    "        \n",
    "        correct += count_same_entries\n",
    "        incorrect += label.shape[0] - count_same_entries\n",
    "\n",
    "    print( \"Correctly predicted words    : \", correct )\n",
    "    print( \"Incorrectly predicted words  : \", incorrect )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdcf1cf-2b06-4dcf-b404-a15f29a5d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # ================ Hyper-parameters ================ #\n",
    "    \n",
    "    batch_size = 64\n",
    "    embedding_size = 16\n",
    "    hidden_size = 25\n",
    "    seq_length = 5      # number of tokens used as a datapoint\n",
    "    learning_rate = 0.001\n",
    "    epochs = 4\n",
    "    \n",
    "\n",
    "    # ====================== Data ===================== #\n",
    "\n",
    "    # select files with text for training (will also be used for test and validation dataset)\n",
    "    #mod_path = Path(__file__).parent.absolute()\n",
    "    #directory = os.path.join(mod_path, \"data/clean_data/\")\n",
    "    #txt_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "    #txt_files = [os.path.basename(file) for file in txt_files]\n",
    "    #txt_files = [\"/Users/kathideckenbach/Documents/Machine Learning Master/Year 1/P4/Language Engineering/Assignments/word_predictor/data/clean_data/twitter.txt\"]\n",
    "    txt_files = [\"twitter.txt\", \"HP_book_1.txt\"]\n",
    "    txt_files = [\"../../data/clean_data/news_summarization.txt\", \"../../data/clean_data/news.txt\",\n",
    "                 \"../../data/clean_data/mobile_text.txt\", \"../../data/clean_data/twitter.txt\"]\n",
    "    txt_files = [\"../../data/clean_data/articles.txt\", \"../../data/clean_data/news_summarization.txt\"]\n",
    "\n",
    "    \n",
    "    # choose tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # set up dataloaders\n",
    "    dataset = WPDataset(filenames=txt_files, tokenizer=tokenizer, n=seq_length)\n",
    "\n",
    "    # split the dataset into train, validation and test set\n",
    "    size_dataset = len(dataset)\n",
    "    datapoints = list(range(size_dataset))\n",
    "    np.random.shuffle(datapoints)\n",
    "    train_split = int(0.8 * size_dataset)\n",
    "    val_split = int(0.05 * size_dataset) + train_split\n",
    "    training_indices = datapoints[:train_split]\n",
    "    validation_indices = datapoints[train_split:val_split]\n",
    "    test_indices =  datapoints[val_split:]\n",
    "\n",
    "    # create a dataloader for each of the sets\n",
    "    training_sampler = SubsetRandomSampler(training_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "\n",
    "    # ==================== Training ==================== #\n",
    "    # Reproducibility\n",
    "    np.random.seed(5719)\n",
    "\n",
    "    device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "    print( \"Running on\", device )\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=training_sampler, drop_last=True) # colate_fn = pad_sequence\n",
    "    val_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=validation_sampler, drop_last=True)\n",
    "    test_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, drop_last=True)\n",
    "    \n",
    "    print( \"There are\", len(dataset), \"datapoints and \", tokenizer.vocab_size, \"unique tokens in the dataset\" ) \n",
    "    \n",
    "\n",
    "    # set up model\n",
    "    rnn_model = RNN(embedding_size, hidden_size, no_of_output_symbols=tokenizer.vocab_size, device=device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    rnn_model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        hidden = None\n",
    "        with tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch + 1)) as tepoch:\n",
    "            for sequence, label in tepoch:\n",
    "                sequence, label = sequence.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits, hidden = rnn_model(sequence, hidden)\n",
    "                hidden = hidden.detach()  # Detach hidden states to avoid backprop through the entire sequence\n",
    "                    \n",
    "                loss = criterion(logits.squeeze(), torch.tensor(label).to(device))\n",
    "                loss.backward()\n",
    "                \n",
    "                #clip_grad_norm_(rnn_model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "                total_loss += loss\n",
    "        print(\"Epoch\", epoch, \"loss:\", total_loss.detach().item() )\n",
    "        total_loss = 0\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Evaluating on the validation data...\")\n",
    "            evaluate(val_dataloader, rnn_model, device)\n",
    "\n",
    "    # ==================== Save the model  ==================== #\n",
    "\n",
    "    dt = str(datetime.now()).replace(' ','_').replace(':','_').replace('.','_')\n",
    "    newdir = 'model_' + dt\n",
    "    os.mkdir( newdir )\n",
    "    torch.save( rnn_model.state_dict(), os.path.join(newdir, 'rnn.model') )\n",
    "\n",
    "    settings = {\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'embedding_size': embedding_size\n",
    "    }\n",
    "    with open( os.path.join(newdir, 'settings.json'), 'w' ) as f:\n",
    "        json.dump(settings, f)\n",
    "\n",
    "    # ==================== Evaluation ==================== #\n",
    "\n",
    "    rnn_model.eval()\n",
    "    print( \"Evaluating on the test data...\" )\n",
    "\n",
    "    print( \"Number of test sentences: \", len(test_dataloader) )\n",
    "    print()\n",
    "\n",
    "    evaluate(test_dataloader, rnn_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed4cae8-8210-4f6e-a859-a68dbdd0183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in  ../../data/clean_data/articles.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 338/338 [00:02<00:00, 141.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in  ../../data/clean_data/news_summarization.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 870522/870522 [03:42<00:00, 3903.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps\n",
      "There are 50512952 datapoints and  30522 unique tokens in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/631411 [00:00<?, ?it/s]/var/folders/pv/8rr_x_n14xj2xz39k_9yqwc00000gn/T/ipykernel_94357/3827770718.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(logits.squeeze(), torch.tensor(label).to(device))\n",
      "Epoch 1:  53%|█████▎    | 335018/631411 [2:12:40<8:42:54,  9.45it/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fee477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
